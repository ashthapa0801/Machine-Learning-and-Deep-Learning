{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgV2gkTfpCc"
      },
      "source": [
        "## On-policy vs. Off-policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning Ïµ-greedy"
      ],
      "metadata": {
        "id": "b0IbPmUsS6kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.activations import softmax\n",
        "from keras.optimizers import Adam\n",
        "import os\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nQcZ7wWyJ2Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A47sXwdmhlrU"
      },
      "source": [
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "## DQNAgent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=20000)\n",
        "    self.gamma = 0.95       # discount rate\n",
        "    self.epsilon = 1.0      # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "\n",
        "\n",
        "  def _build_model(self):\n",
        "    # Neural Net for Deep-Q learning Model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(48, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state):\n",
        "    ## We implement the epsilon-greedy policy\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "        return random.randrange(self.action_size)\n",
        "    act_values = self.model.predict(state, verbose=0)\n",
        "    return np.argmax(act_values[0]) # returns action\n",
        "\n",
        "  def exploit(self, state):\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "    action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "    reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "    next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "    done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "    ### Q-learning\n",
        "    target = (reward_b + self.gamma * np.amax(self.model.predict(next_state_b, verbose=0), 1))\n",
        "    target[done_b==1] = reward_b[done_b==1]\n",
        "    target_f = self.model.predict(state_b, verbose=0)\n",
        "\n",
        "    for k in range(target_f.shape[0]):\n",
        "      target_f[k][action_b[k]] = target[k]\n",
        "    self.model.fit(state_b, target_f, batch_size=state_b.shape[0], verbose=0)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  EPISODES = 200\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  agent = DQNAgent(state_size, action_size)   ## AGENT\n",
        "  batch_size = 32\n",
        "  episode_reward_list = deque(maxlen=50)\n",
        "  q_egreedy_reward_avg50_list = []\n",
        "  for e in range(EPISODES):\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, state_size])\n",
        "      total_reward = 0\n",
        "      for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "      episode_reward_list.append(total_reward)\n",
        "      episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "      q_egreedy_reward_avg50_list.append(episode_reward_avg)\n",
        "      print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                  .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An4lVW3TfHDc",
        "outputId": "fa752ef1-cb2d-4efb-8952-9f788817d28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/200, score: 12.0, e: 1.0, last 50 ep. avg. rew.: 12.00\n",
            "episode: 1/200, score: 14.0, e: 1.0, last 50 ep. avg. rew.: 13.00\n",
            "episode: 2/200, score: 30.0, e: 0.89, last 50 ep. avg. rew.: 18.67\n",
            "episode: 3/200, score: 25.0, e: 0.79, last 50 ep. avg. rew.: 20.25\n",
            "episode: 4/200, score: 17.0, e: 0.73, last 50 ep. avg. rew.: 19.60\n",
            "episode: 5/200, score: 9.0, e: 0.7, last 50 ep. avg. rew.: 17.83\n",
            "episode: 6/200, score: 11.0, e: 0.67, last 50 ep. avg. rew.: 16.86\n",
            "episode: 7/200, score: 10.0, e: 0.64, last 50 ep. avg. rew.: 16.00\n",
            "episode: 8/200, score: 14.0, e: 0.6, last 50 ep. avg. rew.: 15.78\n",
            "episode: 9/200, score: 22.0, e: 0.54, last 50 ep. avg. rew.: 16.40\n",
            "episode: 10/200, score: 10.0, e: 0.51, last 50 ep. avg. rew.: 15.82\n",
            "episode: 11/200, score: 12.0, e: 0.49, last 50 ep. avg. rew.: 15.50\n",
            "episode: 12/200, score: 12.0, e: 0.46, last 50 ep. avg. rew.: 15.23\n",
            "episode: 13/200, score: 33.0, e: 0.39, last 50 ep. avg. rew.: 16.50\n",
            "episode: 14/200, score: 13.0, e: 0.37, last 50 ep. avg. rew.: 16.27\n",
            "episode: 15/200, score: 10.0, e: 0.35, last 50 ep. avg. rew.: 15.88\n",
            "episode: 16/200, score: 12.0, e: 0.33, last 50 ep. avg. rew.: 15.65\n",
            "episode: 17/200, score: 10.0, e: 0.32, last 50 ep. avg. rew.: 15.33\n",
            "episode: 18/200, score: 9.0, e: 0.31, last 50 ep. avg. rew.: 15.00\n",
            "episode: 19/200, score: 11.0, e: 0.29, last 50 ep. avg. rew.: 14.80\n",
            "episode: 20/200, score: 10.0, e: 0.28, last 50 ep. avg. rew.: 14.57\n",
            "episode: 21/200, score: 12.0, e: 0.26, last 50 ep. avg. rew.: 14.45\n",
            "episode: 22/200, score: 9.0, e: 0.25, last 50 ep. avg. rew.: 14.22\n",
            "episode: 23/200, score: 10.0, e: 0.24, last 50 ep. avg. rew.: 14.04\n",
            "episode: 24/200, score: 9.0, e: 0.23, last 50 ep. avg. rew.: 13.84\n",
            "episode: 25/200, score: 13.0, e: 0.22, last 50 ep. avg. rew.: 13.81\n",
            "episode: 26/200, score: 13.0, e: 0.21, last 50 ep. avg. rew.: 13.78\n",
            "episode: 27/200, score: 10.0, e: 0.2, last 50 ep. avg. rew.: 13.64\n",
            "episode: 28/200, score: 15.0, e: 0.18, last 50 ep. avg. rew.: 13.69\n",
            "episode: 29/200, score: 49.0, e: 0.14, last 50 ep. avg. rew.: 14.87\n",
            "episode: 30/200, score: 64.0, e: 0.11, last 50 ep. avg. rew.: 16.45\n",
            "episode: 31/200, score: 29.0, e: 0.092, last 50 ep. avg. rew.: 16.84\n",
            "episode: 32/200, score: 20.0, e: 0.083, last 50 ep. avg. rew.: 16.94\n",
            "episode: 33/200, score: 27.0, e: 0.073, last 50 ep. avg. rew.: 17.24\n",
            "episode: 34/200, score: 21.0, e: 0.066, last 50 ep. avg. rew.: 17.34\n",
            "episode: 35/200, score: 23.0, e: 0.059, last 50 ep. avg. rew.: 17.50\n",
            "episode: 36/200, score: 15.0, e: 0.055, last 50 ep. avg. rew.: 17.43\n",
            "episode: 37/200, score: 23.0, e: 0.049, last 50 ep. avg. rew.: 17.58\n",
            "episode: 38/200, score: 22.0, e: 0.044, last 50 ep. avg. rew.: 17.69\n",
            "episode: 39/200, score: 23.0, e: 0.04, last 50 ep. avg. rew.: 17.82\n",
            "episode: 40/200, score: 30.0, e: 0.034, last 50 ep. avg. rew.: 18.12\n",
            "episode: 41/200, score: 29.0, e: 0.03, last 50 ep. avg. rew.: 18.38\n",
            "episode: 42/200, score: 33.0, e: 0.025, last 50 ep. avg. rew.: 18.72\n",
            "episode: 43/200, score: 80.0, e: 0.017, last 50 ep. avg. rew.: 20.11\n",
            "episode: 44/200, score: 23.0, e: 0.015, last 50 ep. avg. rew.: 20.18\n",
            "episode: 45/200, score: 38.0, e: 0.013, last 50 ep. avg. rew.: 20.57\n",
            "episode: 46/200, score: 41.0, e: 0.01, last 50 ep. avg. rew.: 21.00\n",
            "episode: 47/200, score: 38.0, e: 0.01, last 50 ep. avg. rew.: 21.35\n",
            "episode: 48/200, score: 31.0, e: 0.01, last 50 ep. avg. rew.: 21.55\n",
            "episode: 49/200, score: 45.0, e: 0.01, last 50 ep. avg. rew.: 22.02\n",
            "episode: 50/200, score: 33.0, e: 0.01, last 50 ep. avg. rew.: 22.44\n",
            "episode: 51/200, score: 31.0, e: 0.01, last 50 ep. avg. rew.: 22.78\n",
            "episode: 52/200, score: 68.0, e: 0.01, last 50 ep. avg. rew.: 23.54\n",
            "episode: 53/200, score: 81.0, e: 0.01, last 50 ep. avg. rew.: 24.66\n",
            "episode: 54/200, score: 70.0, e: 0.01, last 50 ep. avg. rew.: 25.72\n",
            "episode: 55/200, score: 22.0, e: 0.01, last 50 ep. avg. rew.: 25.98\n",
            "episode: 56/200, score: 37.0, e: 0.01, last 50 ep. avg. rew.: 26.50\n",
            "episode: 57/200, score: 59.0, e: 0.01, last 50 ep. avg. rew.: 27.48\n",
            "episode: 58/200, score: 41.0, e: 0.01, last 50 ep. avg. rew.: 28.02\n",
            "episode: 59/200, score: 82.0, e: 0.01, last 50 ep. avg. rew.: 29.22\n",
            "episode: 60/200, score: 81.0, e: 0.01, last 50 ep. avg. rew.: 30.64\n",
            "episode: 61/200, score: 39.0, e: 0.01, last 50 ep. avg. rew.: 31.18\n",
            "episode: 62/200, score: 55.0, e: 0.01, last 50 ep. avg. rew.: 32.04\n",
            "episode: 63/200, score: 71.0, e: 0.01, last 50 ep. avg. rew.: 32.80\n",
            "episode: 64/200, score: 92.0, e: 0.01, last 50 ep. avg. rew.: 34.38\n",
            "episode: 65/200, score: 40.0, e: 0.01, last 50 ep. avg. rew.: 34.98\n",
            "episode: 66/200, score: 80.0, e: 0.01, last 50 ep. avg. rew.: 36.34\n",
            "episode: 67/200, score: 46.0, e: 0.01, last 50 ep. avg. rew.: 37.06\n",
            "episode: 68/200, score: 54.0, e: 0.01, last 50 ep. avg. rew.: 37.96\n",
            "episode: 69/200, score: 87.0, e: 0.01, last 50 ep. avg. rew.: 39.48\n",
            "episode: 70/200, score: 91.0, e: 0.01, last 50 ep. avg. rew.: 41.10\n",
            "episode: 71/200, score: 189.0, e: 0.01, last 50 ep. avg. rew.: 44.64\n",
            "episode: 72/200, score: 113.0, e: 0.01, last 50 ep. avg. rew.: 46.72\n",
            "episode: 73/200, score: 165.0, e: 0.01, last 50 ep. avg. rew.: 49.82\n",
            "episode: 74/200, score: 102.0, e: 0.01, last 50 ep. avg. rew.: 51.68\n",
            "episode: 75/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 55.42\n",
            "episode: 76/200, score: 171.0, e: 0.01, last 50 ep. avg. rew.: 58.58\n",
            "episode: 77/200, score: 186.0, e: 0.01, last 50 ep. avg. rew.: 62.10\n",
            "episode: 78/200, score: 159.0, e: 0.01, last 50 ep. avg. rew.: 64.98\n",
            "episode: 79/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 67.20\n",
            "episode: 80/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 69.92\n",
            "episode: 81/200, score: 151.0, e: 0.01, last 50 ep. avg. rew.: 72.36\n",
            "episode: 82/200, score: 146.0, e: 0.01, last 50 ep. avg. rew.: 74.88\n",
            "episode: 83/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 77.54\n",
            "episode: 84/200, score: 166.0, e: 0.01, last 50 ep. avg. rew.: 80.44\n",
            "episode: 85/200, score: 161.0, e: 0.01, last 50 ep. avg. rew.: 83.20\n",
            "episode: 86/200, score: 193.0, e: 0.01, last 50 ep. avg. rew.: 86.76\n",
            "episode: 87/200, score: 145.0, e: 0.01, last 50 ep. avg. rew.: 89.20\n",
            "episode: 88/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 92.76\n",
            "episode: 89/200, score: 146.0, e: 0.01, last 50 ep. avg. rew.: 95.22\n",
            "episode: 90/200, score: 151.0, e: 0.01, last 50 ep. avg. rew.: 97.64\n",
            "episode: 91/200, score: 153.0, e: 0.01, last 50 ep. avg. rew.: 100.12\n",
            "episode: 92/200, score: 152.0, e: 0.01, last 50 ep. avg. rew.: 102.50\n",
            "episode: 93/200, score: 143.0, e: 0.01, last 50 ep. avg. rew.: 103.76\n",
            "episode: 94/200, score: 170.0, e: 0.01, last 50 ep. avg. rew.: 106.70\n",
            "episode: 95/200, score: 150.0, e: 0.01, last 50 ep. avg. rew.: 108.94\n",
            "episode: 96/200, score: 150.0, e: 0.01, last 50 ep. avg. rew.: 111.12\n",
            "episode: 97/200, score: 169.0, e: 0.01, last 50 ep. avg. rew.: 113.74\n",
            "episode: 98/200, score: 158.0, e: 0.01, last 50 ep. avg. rew.: 116.28\n",
            "episode: 99/200, score: 143.0, e: 0.01, last 50 ep. avg. rew.: 118.24\n",
            "episode: 100/200, score: 165.0, e: 0.01, last 50 ep. avg. rew.: 120.88\n",
            "episode: 101/200, score: 155.0, e: 0.01, last 50 ep. avg. rew.: 123.36\n",
            "episode: 102/200, score: 129.0, e: 0.01, last 50 ep. avg. rew.: 124.58\n",
            "episode: 103/200, score: 132.0, e: 0.01, last 50 ep. avg. rew.: 125.60\n",
            "episode: 104/200, score: 159.0, e: 0.01, last 50 ep. avg. rew.: 127.38\n",
            "episode: 105/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 130.14\n",
            "episode: 106/200, score: 140.0, e: 0.01, last 50 ep. avg. rew.: 132.20\n",
            "episode: 107/200, score: 164.0, e: 0.01, last 50 ep. avg. rew.: 134.30\n",
            "episode: 108/200, score: 148.0, e: 0.01, last 50 ep. avg. rew.: 136.44\n",
            "episode: 109/200, score: 133.0, e: 0.01, last 50 ep. avg. rew.: 137.46\n",
            "episode: 110/200, score: 136.0, e: 0.01, last 50 ep. avg. rew.: 138.56\n",
            "episode: 111/200, score: 134.0, e: 0.01, last 50 ep. avg. rew.: 140.46\n",
            "episode: 112/200, score: 175.0, e: 0.01, last 50 ep. avg. rew.: 142.86\n",
            "episode: 113/200, score: 159.0, e: 0.01, last 50 ep. avg. rew.: 144.62\n",
            "episode: 114/200, score: 146.0, e: 0.01, last 50 ep. avg. rew.: 145.70\n",
            "episode: 115/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 148.90\n",
            "episode: 116/200, score: 143.0, e: 0.01, last 50 ep. avg. rew.: 150.16\n",
            "episode: 117/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 152.44\n",
            "episode: 118/200, score: 152.0, e: 0.01, last 50 ep. avg. rew.: 154.40\n",
            "episode: 119/200, score: 165.0, e: 0.01, last 50 ep. avg. rew.: 155.96\n",
            "episode: 120/200, score: 134.0, e: 0.01, last 50 ep. avg. rew.: 156.82\n",
            "episode: 121/200, score: 186.0, e: 0.01, last 50 ep. avg. rew.: 156.76\n",
            "episode: 122/200, score: 138.0, e: 0.01, last 50 ep. avg. rew.: 157.26\n",
            "episode: 123/200, score: 155.0, e: 0.01, last 50 ep. avg. rew.: 157.06\n",
            "episode: 124/200, score: 159.0, e: 0.01, last 50 ep. avg. rew.: 158.20\n",
            "episode: 125/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 157.40\n",
            "episode: 126/200, score: 182.0, e: 0.01, last 50 ep. avg. rew.: 157.62\n",
            "episode: 127/200, score: 165.0, e: 0.01, last 50 ep. avg. rew.: 157.20\n",
            "episode: 128/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 158.02\n",
            "episode: 129/200, score: 167.0, e: 0.01, last 50 ep. avg. rew.: 158.16\n",
            "episode: 130/200, score: 141.0, e: 0.01, last 50 ep. avg. rew.: 156.98\n",
            "episode: 131/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 157.96\n",
            "episode: 132/200, score: 177.0, e: 0.01, last 50 ep. avg. rew.: 158.58\n",
            "episode: 133/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 159.38\n",
            "episode: 134/200, score: 192.0, e: 0.01, last 50 ep. avg. rew.: 159.90\n",
            "episode: 135/200, score: 140.0, e: 0.01, last 50 ep. avg. rew.: 159.48\n",
            "episode: 136/200, score: 176.0, e: 0.01, last 50 ep. avg. rew.: 159.14\n",
            "episode: 137/200, score: 162.0, e: 0.01, last 50 ep. avg. rew.: 159.48\n",
            "episode: 138/200, score: 182.0, e: 0.01, last 50 ep. avg. rew.: 159.12\n",
            "episode: 139/200, score: 167.0, e: 0.01, last 50 ep. avg. rew.: 159.54\n",
            "episode: 140/200, score: 153.0, e: 0.01, last 50 ep. avg. rew.: 159.58\n",
            "episode: 141/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 160.52\n",
            "episode: 142/200, score: 170.0, e: 0.01, last 50 ep. avg. rew.: 160.88\n",
            "episode: 143/200, score: 149.0, e: 0.01, last 50 ep. avg. rew.: 161.00\n",
            "episode: 144/200, score: 193.0, e: 0.01, last 50 ep. avg. rew.: 161.46\n",
            "episode: 145/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 162.46\n",
            "episode: 146/200, score: 155.0, e: 0.01, last 50 ep. avg. rew.: 162.56\n",
            "episode: 147/200, score: 152.0, e: 0.01, last 50 ep. avg. rew.: 162.22\n",
            "episode: 148/200, score: 141.0, e: 0.01, last 50 ep. avg. rew.: 161.88\n",
            "episode: 149/200, score: 155.0, e: 0.01, last 50 ep. avg. rew.: 162.12\n",
            "episode: 150/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 162.82\n",
            "episode: 151/200, score: 150.0, e: 0.01, last 50 ep. avg. rew.: 162.72\n",
            "episode: 152/200, score: 158.0, e: 0.01, last 50 ep. avg. rew.: 163.30\n",
            "episode: 153/200, score: 197.0, e: 0.01, last 50 ep. avg. rew.: 164.60\n",
            "episode: 154/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 165.42\n",
            "episode: 155/200, score: 151.0, e: 0.01, last 50 ep. avg. rew.: 165.24\n",
            "episode: 156/200, score: 153.0, e: 0.01, last 50 ep. avg. rew.: 165.50\n",
            "episode: 157/200, score: 164.0, e: 0.01, last 50 ep. avg. rew.: 165.50\n",
            "episode: 158/200, score: 167.0, e: 0.01, last 50 ep. avg. rew.: 165.88\n",
            "episode: 159/200, score: 194.0, e: 0.01, last 50 ep. avg. rew.: 167.10\n",
            "episode: 160/200, score: 127.0, e: 0.01, last 50 ep. avg. rew.: 166.92\n",
            "episode: 161/200, score: 139.0, e: 0.01, last 50 ep. avg. rew.: 167.02\n",
            "episode: 162/200, score: 181.0, e: 0.01, last 50 ep. avg. rew.: 167.14\n",
            "episode: 163/200, score: 187.0, e: 0.01, last 50 ep. avg. rew.: 167.70\n",
            "episode: 164/200, score: 199.0, e: 0.01, last 50 ep. avg. rew.: 168.76\n",
            "episode: 165/200, score: 153.0, e: 0.01, last 50 ep. avg. rew.: 167.82\n",
            "episode: 166/200, score: 148.0, e: 0.01, last 50 ep. avg. rew.: 167.92\n",
            "episode: 167/200, score: 133.0, e: 0.01, last 50 ep. avg. rew.: 167.38\n",
            "episode: 168/200, score: 123.0, e: 0.01, last 50 ep. avg. rew.: 166.80\n",
            "episode: 169/200, score: 181.0, e: 0.01, last 50 ep. avg. rew.: 167.12\n",
            "episode: 170/200, score: 35.0, e: 0.01, last 50 ep. avg. rew.: 165.14\n",
            "episode: 171/200, score: 188.0, e: 0.01, last 50 ep. avg. rew.: 165.18\n",
            "episode: 172/200, score: 145.0, e: 0.01, last 50 ep. avg. rew.: 165.32\n",
            "episode: 173/200, score: 188.0, e: 0.01, last 50 ep. avg. rew.: 165.98\n",
            "episode: 174/200, score: 135.0, e: 0.01, last 50 ep. avg. rew.: 165.50\n",
            "episode: 175/200, score: 142.0, e: 0.01, last 50 ep. avg. rew.: 165.14\n",
            "episode: 176/200, score: 155.0, e: 0.01, last 50 ep. avg. rew.: 164.60\n",
            "episode: 177/200, score: 100.0, e: 0.01, last 50 ep. avg. rew.: 163.30\n",
            "episode: 178/200, score: 160.0, e: 0.01, last 50 ep. avg. rew.: 162.50\n",
            "episode: 179/200, score: 135.0, e: 0.01, last 50 ep. avg. rew.: 161.86\n",
            "episode: 180/200, score: 146.0, e: 0.01, last 50 ep. avg. rew.: 161.96\n",
            "episode: 181/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 161.96\n",
            "episode: 182/200, score: 164.0, e: 0.01, last 50 ep. avg. rew.: 161.70\n",
            "episode: 183/200, score: 151.0, e: 0.01, last 50 ep. avg. rew.: 160.72\n",
            "episode: 184/200, score: 190.0, e: 0.01, last 50 ep. avg. rew.: 160.68\n",
            "episode: 185/200, score: 164.0, e: 0.01, last 50 ep. avg. rew.: 161.16\n",
            "episode: 186/200, score: 142.0, e: 0.01, last 50 ep. avg. rew.: 160.48\n",
            "episode: 187/200, score: 165.0, e: 0.01, last 50 ep. avg. rew.: 160.54\n",
            "episode: 188/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 160.90\n",
            "episode: 189/200, score: 138.0, e: 0.01, last 50 ep. avg. rew.: 160.32\n",
            "episode: 190/200, score: 171.0, e: 0.01, last 50 ep. avg. rew.: 160.68\n",
            "episode: 191/200, score: 191.0, e: 0.01, last 50 ep. avg. rew.: 160.50\n",
            "episode: 192/200, score: 158.0, e: 0.01, last 50 ep. avg. rew.: 160.26\n",
            "episode: 193/200, score: 157.0, e: 0.01, last 50 ep. avg. rew.: 160.42\n",
            "episode: 194/200, score: 171.0, e: 0.01, last 50 ep. avg. rew.: 159.98\n",
            "episode: 195/200, score: 154.0, e: 0.01, last 50 ep. avg. rew.: 159.06\n",
            "episode: 196/200, score: 159.0, e: 0.01, last 50 ep. avg. rew.: 159.14\n",
            "episode: 197/200, score: 146.0, e: 0.01, last 50 ep. avg. rew.: 159.02\n",
            "episode: 198/200, score: 200.0, e: 0.01, last 50 ep. avg. rew.: 160.20\n",
            "episode: 199/200, score: 136.0, e: 0.01, last 50 ep. avg. rew.: 159.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning softmax"
      ],
      "metadata": {
        "id": "HNMebiR4UQ1l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h4YDin3UQ1m"
      },
      "source": [
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "## DQNAgent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=20000)\n",
        "    self.gamma = 0.95       # discount rate\n",
        "    self.epsilon = 1.0      # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "\n",
        "\n",
        "  def _build_model(self):\n",
        "    # Neural Net for Deep-Q learning Model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(48, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state):\n",
        "    ## Softmax policy\n",
        "    act_values = self.model.predict(state, verbose=0)\n",
        "    p = softmax(act_values)[0]\n",
        "    p = p / p.sum()\n",
        "    return np.random.choice(np.arange(self.action_size), p=p) # returns action\n",
        "\n",
        "  def exploit(self, state):\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  ## MODIFY REPLAY\n",
        "  def replay(self, batch_size):\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "    action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "    reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "    next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "    done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "    ### Q-learning\n",
        "    target = (reward_b + self.gamma * np.amax(self.model.predict(next_state_b, verbose=0), 1))\n",
        "    target[done_b==1] = reward_b[done_b==1]\n",
        "    target_f = self.model.predict(state_b, verbose=0)\n",
        "\n",
        "    for k in range(target_f.shape[0]):\n",
        "      target_f[k][action_b[k]] = target[k]\n",
        "    self.model.fit(state_b, target_f, batch_size=state_b.shape[0], verbose=0)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "3c48d846-9871-437b-b9a0-d1cf15c652a0",
        "id": "MaQBO_47UQ1m"
      },
      "source": [
        "  EPISODES = 200\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  agent = DQNAgent(state_size, action_size)   ## AGENT\n",
        "  batch_size = 32\n",
        "  episode_reward_list = deque(maxlen=50)\n",
        "  q_softmax_reward_avg50_list = []\n",
        "  for e in range(EPISODES):\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, state_size])\n",
        "      total_reward = 0\n",
        "      for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "      episode_reward_list.append(total_reward)\n",
        "      episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "      q_softmax_reward_avg50_list.append(episode_reward_avg)\n",
        "      print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                  .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/200, score: 9.0, e: 1.0, last 50 ep. avg. rew.: 9.00\n",
            "episode: 1/200, score: 10.0, e: 1.0, last 50 ep. avg. rew.: 9.50\n",
            "episode: 2/200, score: 10.0, e: 1.0, last 50 ep. avg. rew.: 9.67\n",
            "episode: 3/200, score: 11.0, e: 0.97, last 50 ep. avg. rew.: 10.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2dda4f3e297b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mepisode_reward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepisode_reward_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1b0e3d278a4a>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA Ïµ-greedy"
      ],
      "metadata": {
        "id": "waMZEKRBUSuG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IfV2CkKUSuG"
      },
      "source": [
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "## DQNAgent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=20000)\n",
        "    self.gamma = 0.95       # discount rate\n",
        "    self.epsilon = 1.0      # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "\n",
        "\n",
        "  def _build_model(self):\n",
        "    # Neural Net for Deep-Q learning Model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(48, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state):\n",
        "    ## We implement the epsilon-greedy policy\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "        return random.randrange(self.action_size)\n",
        "    act_values = self.model.predict(state, verbose=0)\n",
        "    return np.argmax(act_values[0]) # returns action\n",
        "\n",
        "  def exploit(self, state):\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "    action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "    reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "    next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "    done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "    ### SARSA\n",
        "    # For each next state in a batch, calculate the next action based on curent policy\n",
        "    next_action_b = np.zeros(next_state_b.shape[0]).astype(int)\n",
        "    for i in range(next_state_b.shape[0]):\n",
        "      next_action_b[i] = self.act(next_state_b[i].reshape((1,-1)))\n",
        "    # Estimate the action-values for the batch of next states and actions\n",
        "    next_q_b = self.model.predict(next_state_b, verbose=0)\n",
        "    next_q_b = next_q_b[np.arange(next_q_b.shape[0]), next_action_b]\n",
        "    # SARSA update rule\n",
        "    target = (reward_b + self.gamma * next_q_b)\n",
        "    target[done_b==1] = reward_b[done_b==1]\n",
        "    target_f = self.model.predict(state_b, verbose=0)\n",
        "\n",
        "    for k in range(target_f.shape[0]):\n",
        "      target_f[k][action_b[k]] = target[k]\n",
        "    self.model.fit(state_b, target_f, batch_size=state_b.shape[0], verbose=0)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecd1bb45-ab9d-4efb-92e8-f0731ab639b9",
        "id": "A_-VlRuBUSuH"
      },
      "source": [
        "  EPISODES = 200\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  agent = DQNAgent(state_size, action_size)   ## AGENT\n",
        "  batch_size = 32\n",
        "  episode_reward_list = deque(maxlen=50)\n",
        "  sarsa_egreedy_reward_avg50_list = []\n",
        "  for e in range(EPISODES):\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, state_size])\n",
        "      total_reward = 0\n",
        "      for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "      episode_reward_list.append(total_reward)\n",
        "      episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "      sarsa_egreedy_reward_avg50_list.append(episode_reward_avg)\n",
        "      print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                  .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/200, score: 12.0, e: 1.0, last 50 ep. avg. rew.: 12.00\n",
            "episode: 1/200, score: 14.0, e: 1.0, last 50 ep. avg. rew.: 13.00\n",
            "episode: 2/200, score: 12.0, e: 0.98, last 50 ep. avg. rew.: 12.67\n",
            "episode: 3/200, score: 15.0, e: 0.91, last 50 ep. avg. rew.: 13.25\n",
            "episode: 4/200, score: 10.0, e: 0.87, last 50 ep. avg. rew.: 12.60\n",
            "episode: 5/200, score: 22.0, e: 0.78, last 50 ep. avg. rew.: 14.17\n",
            "episode: 6/200, score: 14.0, e: 0.73, last 50 ep. avg. rew.: 14.14\n",
            "episode: 7/200, score: 17.0, e: 0.68, last 50 ep. avg. rew.: 14.50\n",
            "episode: 8/200, score: 11.0, e: 0.64, last 50 ep. avg. rew.: 14.11\n",
            "episode: 9/200, score: 21.0, e: 0.58, last 50 ep. avg. rew.: 14.80\n",
            "episode: 10/200, score: 37.0, e: 0.49, last 50 ep. avg. rew.: 16.82\n",
            "episode: 11/200, score: 14.0, e: 0.46, last 50 ep. avg. rew.: 16.58\n",
            "episode: 12/200, score: 10.0, e: 0.44, last 50 ep. avg. rew.: 16.08\n",
            "episode: 13/200, score: 17.0, e: 0.4, last 50 ep. avg. rew.: 16.14\n",
            "episode: 14/200, score: 10.0, e: 0.38, last 50 ep. avg. rew.: 15.73\n",
            "episode: 15/200, score: 12.0, e: 0.36, last 50 ep. avg. rew.: 15.50\n",
            "episode: 16/200, score: 17.0, e: 0.34, last 50 ep. avg. rew.: 15.59\n",
            "episode: 17/200, score: 9.0, e: 0.32, last 50 ep. avg. rew.: 15.22\n",
            "episode: 18/200, score: 22.0, e: 0.29, last 50 ep. avg. rew.: 15.58\n",
            "episode: 19/200, score: 11.0, e: 0.28, last 50 ep. avg. rew.: 15.35\n",
            "episode: 20/200, score: 12.0, e: 0.26, last 50 ep. avg. rew.: 15.19\n",
            "episode: 21/200, score: 13.0, e: 0.25, last 50 ep. avg. rew.: 15.09\n",
            "episode: 22/200, score: 10.0, e: 0.23, last 50 ep. avg. rew.: 14.87\n",
            "episode: 23/200, score: 9.0, e: 0.23, last 50 ep. avg. rew.: 14.62\n",
            "episode: 24/200, score: 11.0, e: 0.21, last 50 ep. avg. rew.: 14.48\n",
            "episode: 25/200, score: 14.0, e: 0.2, last 50 ep. avg. rew.: 14.46\n",
            "episode: 26/200, score: 13.0, e: 0.19, last 50 ep. avg. rew.: 14.41\n",
            "episode: 27/200, score: 12.0, e: 0.18, last 50 ep. avg. rew.: 14.32\n",
            "episode: 28/200, score: 19.0, e: 0.16, last 50 ep. avg. rew.: 14.48\n",
            "episode: 29/200, score: 28.0, e: 0.14, last 50 ep. avg. rew.: 14.93\n",
            "episode: 30/200, score: 29.0, e: 0.12, last 50 ep. avg. rew.: 15.39\n",
            "episode: 31/200, score: 25.0, e: 0.11, last 50 ep. avg. rew.: 15.69\n",
            "episode: 32/200, score: 13.0, e: 0.1, last 50 ep. avg. rew.: 15.61\n",
            "episode: 33/200, score: 16.0, e: 0.096, last 50 ep. avg. rew.: 15.62\n",
            "episode: 34/200, score: 31.0, e: 0.083, last 50 ep. avg. rew.: 16.06\n",
            "episode: 35/200, score: 38.0, e: 0.069, last 50 ep. avg. rew.: 16.67\n",
            "episode: 36/200, score: 25.0, e: 0.061, last 50 ep. avg. rew.: 16.89\n",
            "episode: 37/200, score: 11.0, e: 0.058, last 50 ep. avg. rew.: 16.74\n",
            "episode: 38/200, score: 14.0, e: 0.054, last 50 ep. avg. rew.: 16.67\n",
            "episode: 39/200, score: 11.0, e: 0.052, last 50 ep. avg. rew.: 16.52\n",
            "episode: 40/200, score: 24.0, e: 0.046, last 50 ep. avg. rew.: 16.71\n",
            "episode: 41/200, score: 22.0, e: 0.041, last 50 ep. avg. rew.: 16.83\n",
            "episode: 42/200, score: 21.0, e: 0.038, last 50 ep. avg. rew.: 16.93\n",
            "episode: 43/200, score: 45.0, e: 0.03, last 50 ep. avg. rew.: 17.57\n",
            "episode: 44/200, score: 34.0, e: 0.025, last 50 ep. avg. rew.: 17.93\n",
            "episode: 45/200, score: 51.0, e: 0.02, last 50 ep. avg. rew.: 18.65\n",
            "episode: 46/200, score: 14.0, e: 0.019, last 50 ep. avg. rew.: 18.55\n",
            "episode: 47/200, score: 68.0, e: 0.013, last 50 ep. avg. rew.: 19.58\n",
            "episode: 48/200, score: 48.0, e: 0.01, last 50 ep. avg. rew.: 20.16\n",
            "episode: 49/200, score: 17.0, e: 0.01, last 50 ep. avg. rew.: 20.10\n",
            "episode: 50/200, score: 56.0, e: 0.01, last 50 ep. avg. rew.: 20.98\n",
            "episode: 51/200, score: 31.0, e: 0.01, last 50 ep. avg. rew.: 21.32\n",
            "episode: 52/200, score: 55.0, e: 0.01, last 50 ep. avg. rew.: 22.18\n",
            "episode: 53/200, score: 123.0, e: 0.01, last 50 ep. avg. rew.: 24.34\n",
            "episode: 54/200, score: 114.0, e: 0.01, last 50 ep. avg. rew.: 26.42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-108c58bad02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mepisode_reward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepisode_reward_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-be65229446fc>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mnext_action_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mnext_action_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Estimate the action-values for the batch of next states and actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mnext_q_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-be65229446fc>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 703\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    740\u001b[0m             self._flat_output_types)\n\u001b[1;32m    741\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3410\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARSA softmax"
      ],
      "metadata": {
        "id": "zq4NT7-FUTUR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTPUgHTfUTUS"
      },
      "source": [
        "\n",
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "## DQNAgent\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=20000)\n",
        "    self.gamma = 0.95       # discount rate\n",
        "    self.epsilon = 1.0      # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "\n",
        "\n",
        "  def _build_model(self):\n",
        "    # Neural Net for Deep-Q learning Model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "    model.add(Dense(48, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=Adam(lr=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state):\n",
        "    ## Softmax policy\n",
        "    act_values = self.model.predict(state, verbose=0)\n",
        "    p = softmax(act_values)[0]\n",
        "    p = p / p.sum()\n",
        "    return np.random.choice(np.arange(self.action_size), p=p) # returns action\n",
        "\n",
        "  def exploit(self, state):\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def replay(self, batch_size):\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "    action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "    reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "    next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "    done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "    ### SARSA\n",
        "    # For each next state in a batch, calculate the next action based on curent policy\n",
        "    next_action_b = np.zeros(next_state_b.shape[0]).astype(int)\n",
        "    for i in range(next_state_b.shape[0]):\n",
        "      next_action_b[i] = self.act(next_state_b[i].reshape((1,-1)))\n",
        "    # Estimate the action-values for the batch of next states and actions\n",
        "    next_q_b = self.model.predict(next_state_b, verbose=0)\n",
        "    next_q_b = next_q_b[np.arange(next_q_b.shape[0]), next_action_b]\n",
        "    # SARSA update rule\n",
        "    target = (reward_b + self.gamma * next_q_b)\n",
        "    target[done_b==1] = reward_b[done_b==1]\n",
        "    target_f = self.model.predict(state_b, verbose=0)\n",
        "\n",
        "    for k in range(target_f.shape[0]):\n",
        "      target_f[k][action_b[k]] = target[k]\n",
        "    self.model.fit(state_b, target_f, batch_size=state_b.shape[0], verbose=0)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVppul3XUTUS"
      },
      "source": [
        "  EPISODES = 200\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  agent = DQNAgent(state_size, action_size)   ## AGENT\n",
        "  batch_size = 32\n",
        "  episode_reward_list = deque(maxlen=50)\n",
        "  sarsa_softmax_reward_avg50_list = []\n",
        "  for e in range(EPISODES):\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, state_size])\n",
        "      total_reward = 0\n",
        "      for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "      episode_reward_list.append(total_reward)\n",
        "      episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "      sarsa_softmax_reward_avg50_list.append(episode_reward_avg)\n",
        "      print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                  .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average reward for the last 50 episodes vs. number of training episodes"
      ],
      "metadata": {
        "id": "cKsT5o0j3R2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the average reward for the last 50 episodes vs. number of training episodes\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(q_egreedy_reward_avg50_list, label='Q-learning Ïµ-greedy')\n",
        "plt.plot(q_softmax_reward_avg50_list, label='Q-learning softmax')\n",
        "plt.plot(sarsa_egreedy_reward_avg50_list, label='SARSA Ïµ-greedy')\n",
        "plt.plot(sarsa_softmax_reward_avg50_list, label='SARSA softmax')\n",
        "plt.legend()\n",
        "plt.xlabel('Training episodes')\n",
        "plt.ylabel('Average reward for the last 50 episodes')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gw-f30Mw3CGH",
        "outputId": "931fd866-2814-4e3d-9b0d-4507b68d4a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}